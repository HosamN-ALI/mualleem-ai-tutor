# ğŸš€ Requesty.ai Integration Guide

## Overview
ØªÙ… ØªÙƒÙˆÙŠÙ† Ù…Ù†ØµØ© Ù…Ø¹Ù„Ù‘Ù… Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… **Requesty.ai** ÙƒØ¨ÙˆØ§Ø¨Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 300 Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡.

Your Mualleem platform is now configured to use **Requesty.ai** as a unified AI gateway, providing access to 300+ AI models with built-in optimization, caching, and cost tracking.

---

## âœ… Configuration Status

### API Credentials
- **API Key**: `rqsty-sk-y4aKgcDPSLuXh6PXd4vHGBtHPlWRkyfZVcN6R3thk+7q8djI+bZs0L98Ud0PdZr0rsx1M/N1AGP07BZDhyeDSfVyyhum2Hbf6uVTPyFN8wU=`
- **Base URL**: `https://router.requesty.ai/v1`
- **Site URL**: `http://localhost:3000`
- **Site Name**: `Mualleem - AI Tutoring Platform`

### Models in Use
1. **Chat Completion**:
   - `openai/gpt-4o` - For vision tasks (image + text)
   - `openai/gpt-4o-mini` - For text-only questions (cost-effective)

2. **Embeddings**:
   - `openai/text-embedding-3-small` - For RAG (supports Arabic)

---

## ğŸ“ Updated Files

### 1. `/backend/.env`
```env
REQUESTY_API_KEY=rqsty-sk-y4aKgcDPSLuXh6PXd4vHGBtHPlWRkyfZVcN6R3thk+7q8djI+bZs0L98Ud0PdZr0rsx1M/N1AGP07BZDhyeDSfVyyhum2Hbf6uVTPyFN8wU=
REQUESTY_BASE_URL=https://router.requesty.ai/v1
SITE_URL=http://localhost:3000
SITE_NAME=Mualleem - AI Tutoring Platform
```

### 2. `/backend/rag_service.py`
- âœ… OpenAI client initialized with Requesty.ai base URL
- âœ… Custom headers added (`HTTP-Referer`, `X-Title`)
- âœ… Model format updated to `provider/model` (e.g., `openai/gpt-4o`)

### 3. `/backend/main.py`
- âœ… Chat endpoint uses `openai/gpt-4o` for vision
- âœ… Chat endpoint uses `openai/gpt-4o-mini` for text-only
- âœ… Model names returned in API responses updated

---

## ğŸ§ª Testing the Integration

### Run the Test Script
```bash
cd backend
python test_requesty.py
```

**Expected Output:**
```
ğŸ” Testing Requesty.ai Configuration...

âœ“ API Key: rqsty-sk-y4aKgcDPSL...N8wU=
âœ“ Base URL: https://router.requesty.ai/v1
âœ“ Site URL: http://localhost:3000
âœ“ Site Name: Mualleem

ğŸ“¡ Testing Chat Completion (GPT-4o-mini)...
âœ… Chat Response: Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ

ğŸ“Š Testing Embeddings (text-embedding-3-small)...
âœ… Embedding Generated: 1536 dimensions

ğŸ‰ All tests passed! Requesty.ai is configured correctly.
```

---

## ğŸ”§ How It Works

### Client Initialization (rag_service.py)
```python
from openai import OpenAI

client = OpenAI(
    api_key=requesty_api_key,
    base_url="https://router.requesty.ai/v1",
    default_headers={
        "HTTP-Referer": "http://localhost:3000",
        "X-Title": "Mualleem - AI Tutoring Platform"
    }
)
```

### Chat Completion Example
```python
response = client.chat.completions.create(
    model="openai/gpt-4o-mini",  # Note: provider/model format
    messages=[
        {"role": "user", "content": "Ù…Ø§ Ù‡Ùˆ Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© x + 5 = 10ØŸ"}
    ]
)
```

### Embeddings Example
```python
response = client.embeddings.create(
    model="openai/text-embedding-3-small",
    input=["Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„ØªØ¶Ù…ÙŠÙ†"]
)
```

---

## ğŸ’¡ Benefits of Requesty.ai

1. **Unified Gateway**: Access 300+ models through one API
2. **Cost Optimization**: Automatic routing to cost-effective models
3. **Caching**: Built-in response caching for repeated queries
4. **Monitoring**: Track usage and costs in the Requesty dashboard
5. **Fallback**: Automatic failover if a model is unavailable
6. **Arabic Support**: Full support for RTL languages

---

## ğŸš€ Next Steps

### 1. Start the Backend Server
```bash
cd backend
python -m pip install -r requirements.txt
python main.py
```

### 2. Test the API Endpoints

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Chat Request:**
```bash
curl -X POST http://localhost:8000/chat \
  -F "question=Ù…Ø§ Ù‡Ùˆ 2 + 2ØŸ"
```

### 3. Monitor Usage
Visit [app.requesty.ai](https://app.requesty.ai) to:
- View API usage statistics
- Track costs per model
- Monitor response times
- Set up alerts and limits

---

## ğŸ”’ Security Notes

1. **API Key Protection**: Never commit `.env` file to git
2. **Environment Variables**: Always load from `.env` in production
3. **Rate Limiting**: Requesty.ai handles rate limiting automatically
4. **HTTPS**: All requests are encrypted via HTTPS

---

## ğŸ“š Additional Resources

- [Requesty.ai Documentation](https://docs.requesty.ai)
- [OpenAI SDK Compatibility](https://docs.requesty.ai/integration/openai-sdk)
- [Model Pricing](https://app.requesty.ai/pricing)
- [API Reference](https://docs.requesty.ai/api-reference)

---

## â“ Troubleshooting

### Error: "Invalid API Key"
- Verify the API key in `.env` file
- Check for extra spaces or line breaks
- Regenerate key at app.requesty.ai

### Error: "Model not found"
- Ensure model format is `provider/model` (e.g., `openai/gpt-4o`)
- Check available models at app.requesty.ai

### Error: "Connection timeout"
- Check internet connection
- Verify base URL: `https://router.requesty.ai/v1`
- Check firewall settings

---

## ğŸ“ Support

For issues or questions:
- **Requesty Support**: support@requesty.ai
- **Documentation**: https://docs.requesty.ai
- **Dashboard**: https://app.requesty.ai

---

**ØªÙ… Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­! âœ…**
**Configuration Complete! âœ…**
